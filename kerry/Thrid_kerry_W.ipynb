{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyM9PlnghYglzLK9ADxsRFtq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"oWHNQjS27r9f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666172240735,"user_tz":-480,"elapsed":21042,"user":{"displayName":"yan wang","userId":"02820627279848842686"}},"outputId":"5cd7eca5-76bd-446b-bde0-3a8aca2f60db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import numpy as np\n","#import matplotlib.pyplot as plt\n","!pip install tf_slim"],"metadata":{"id":"2BBnJTDB8NZp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666172244394,"user_tz":-480,"elapsed":3669,"user":{"displayName":"yan wang","userId":"02820627279848842686"}},"outputId":"4cde407d-f4f3-4ce5-93e2-d876eb2836f4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tf_slim\n","  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","\u001b[K     |████████████████████████████████| 352 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (1.3.0)\n","Installing collected packages: tf-slim\n","Successfully installed tf-slim-1.1.0\n"]}]},{"cell_type":"code","source":["from __future__ import print_function\n","from __future__ import division\n","import gc\n","import os\n","import sys\n","import numpy as np\n","import tensorflow.compat.v1 as tf\n","from PIL import Image\n","import argparse\n","from glob import glob\n","import time\n","import natsort\n","import scipy.io as scio\n","import tf_slim as slim\n","import matplotlib.pyplot as plt\n","#!pip install tf_slim\n","\n","def cal_psnr(im1, im2): \n","    # assert pixel value range is 0-255 and type is uint8\n","    snr = -20 * np.log10(np.linalg.norm(im1.flatten()-im2.flatten())/np.linalg.norm(im2.flatten()))\n","    mse = (((im1.astype(np.float) - im2.astype(np.float)) ** 2)/((im2.astype(np.float))**2)).mean()\n","    psnr = -20 * np.log10(mse)\n","    return snr\n","\n","\n","def tf_psnr(im1, im2):\n","    # assert pixel value range is 0-1\n","    mse = tf.losses.mean_squared_error(labels=im2 * 255.0, predictions=im1 * 255.0)\n","    return 10.0 * (tf.log(255.0 ** 2 / mse) / tf.log(10.0))\n","\n","def upsample_and_sum(x1, x2, output_channels, in_channels, scope=None):\n","    pool_size = 2\n","    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02), name=scope)#0.02\n","    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n","    deconv_output = deconv + x2\n","    deconv_output.set_shape([None, None, None, output_channels])\n","    return deconv_output\n","\n","\n","def FCN(input):\n","    with tf.variable_scope('fcn'):\n","        conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv1')\n","        conv2 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv2')\n","        conv3 = slim.conv2d(conv2, 32, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv3')\n","        conv4 = slim.conv2d(conv3, 32, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv4')\n","        conv5 = slim.conv2d(conv4, 1, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv5')\n","        return conv5\n","\n","def UNet(input):\n","    with tf.variable_scope('unet'):\n","        conv1 = slim.conv2d(input, 64, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv1_1')\n","        conv1 = slim.conv2d(conv1, 64, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv1_2')\n","        pool1 = slim.avg_pool2d(conv1, [2, 2], padding='SAME')\n","        conv2 = slim.conv2d(pool1, 128, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv2_1')\n","        conv2 = slim.conv2d(conv2, 128, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv2_2')\n","        conv2 = slim.conv2d(conv2, 128, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv2_3')\n","        pool2 = slim.avg_pool2d(conv2, [2, 2], padding='SAME')\n","        conv3 = slim.conv2d(pool2, 256, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv3_1')\n","        conv3 = slim.conv2d(conv3, 256, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv3_2')\n","        conv3 = slim.conv2d(conv3, 256, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv3_3')\n","        conv3 = slim.conv2d(conv3, 256, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv3_4')\n","        conv3 = slim.conv2d(conv3, 256, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv3_5')\n","        conv3 = slim.conv2d(conv3, 256, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv3_6')\n","        conv3 = slim.conv2d(conv3, 256, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv3_7')#\n","        conv3 = slim.conv2d(conv3, 256, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv3_9')#\n","        conv3 = slim.conv2d(conv3, 256, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv3_10')#\n","        up4 = upsample_and_sum(conv3, conv2, 128, 256, scope='deconv4')\n","        conv4 = slim.conv2d(up4, 128, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv4_1')\n","        conv4 = slim.conv2d(conv4, 128, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv4_2')\n","        conv4 = slim.conv2d(conv4, 128, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv4_3')\n","        up5 = upsample_and_sum(conv4, conv1, 64, 128, scope='deconv5')\n","        conv5 = slim.conv2d(up5, 64, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv5_1')\n","        conv5 = slim.conv2d(conv5, 64, [3, 3], rate=1, activation_fn=tf.nn.relu, scope='conv5_2')\n","        out = slim.conv2d(conv5, 1, [1, 1], rate=1, activation_fn=None, scope='conv6')\n","        return out\n","\n","def dncnn(input,is_training=True):\n","    noise_level = FCN(input)\n","    concat_img = tf.concat([input, noise_level], 3)\n","    out = UNet(concat_img) + input\n","    return out\n","\n","class denoiser(object):\n","    def __init__(self, sess, sigma, cost_str, ckpt_dir, sample_dir, log_dir):\n","        self.sess = sess\n","        self.sigma = sigma\n","        \n","        self.ckpt_dir = ckpt_dir\n","        self.sample_dir = sample_dir\n","        self.log_dir = log_dir\n","        \n","        if not os.path.exists(self.sample_dir):\n","            os.makedirs(self.sample_dir)\n","    \n","        # build model\n","        #placeholders for clean and noisy image batches\n","     #   self.GT = tf.placeholder(tf.float32, [None, None, None, 1], name='gt_true_image')\n","        self.Y = tf.placeholder(tf.float32, [None, None, None, 1], name='noisy_image')\n","       \n","        self.is_training = tf.placeholder(tf.bool, name='is_training') #for batchnorm\n","\n","        #forward propagation\n","        with tf.variable_scope('DnCNN'):\n","            self.Y_ = dncnn(self.Y, is_training=self.is_training)\n","\n","        batch = tf.to_float(tf.shape(self.Y)[0])    #size of the minibatch\n","        \n","\n","        self.noisy = self.Y_ - self.Y\n","\n","        self.Tv_y,_  = tf.image.image_gradients(self.noisy)\n","        _,self.Tv_x  = tf.image.image_gradients(self.Y_)\n","\n","        self.Tv_x = 5e4*tf.reduce_mean(tf.abs(self.Tv_x)) #Along the noise direction　#tf.reduce_mean(total_variation_x(self.noisy))\n","        self.Tv_y = 3e5*tf.reduce_mean(tf.abs(self.Tv_y)) #Vertical noise direction #tf.reduce_mean(total_variation_y(self.Y_))\n","        #dy_true, dx_true = tf.image.image_gradients(y_true)\n","        #dy_pred, dx_pred = tf.image.image_gradients(y_pred)\n","\n","        # MSE\n","        self.mse = (1.0 / batch)*(tf.nn.l2_loss(self.Y - self.Y_) + self.Tv_x + self.Tv_y)\n","\n","\n","        #self.sure     = (1.0 / batch)*(tf.nn.l2_loss(self.Y - self.Y_) - self.var_sum_Y + self.divergence_sum_Y + 1e4*tf.reduce_mean(tf.square(tf.image.image_gradients(self.Y_))))\n","        self.cost = self.mse\n","\n","        \n","        self.lr = tf.placeholder(tf.float32, name='learning_rate')\n","        self.eva_psnr = tf.placeholder(tf.float32, name='eva_psnr')\n","\n","        #optimizer\n","        optimizer = tf.train.AdamOptimizer(self.lr, name='AdamOptimizer')\n","\n","        #for batchnorm\n","        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","        with tf.control_dependencies(update_ops):\n","            self.train_op = optimizer.minimize(self.cost) ##CHANGED\n","\n","        #checkpoint saver\n","        self.saver = tf.train.Saver(max_to_keep=15)\n","\n","        init = tf.global_variables_initializer()\n","        self.sess.run(init)\n","        print(\"[*] Initialize model successfully...\")\n","\n","   #function to evaluate the performance after each epoch\n","\n","    def train(self, data_path, eval_files, batch_size, epoch_, lr, gt_type):\n","        \n","        # CLEAN ground-truth trainset\n","     #   gt_data= np.load('/content/drive/MyDrive/SURE_pstm/pstm_xiaobo.npy')\n","        # NOISY patches trainset\n","        #noisy_name   ='rgb_noisy_' + str(gt_type) + '_patches.npy'\n","        #noisy        = np.load(os.path.join(data_path, noisy_name), mmap_mode='r')    \n","        noisy = np.load('/content/drive/MyDrive/yyh/feature_select/test/kerry_440_400_50_1_W.npy')\n","\n","        numData = np.shape(noisy)[0]\n","        numBatch = int(numData / batch_size)\n","\n","        # load pretrained model\n","       # load_model_status, global_step = self.load(self.ckpt_dir) \n","        load_model_status = False\n","        if load_model_status:\n","            iter_num = global_step\n","            start_epoch = (global_step) // numBatch\n","            start_step = (global_step) % numBatch\n","            print(\"[*] Model restore success!\")\n","        else:\n","            iter_num    = 0\n","            start_epoch = 0\n","            start_step  = 0\n","            print(\"[*] Not find pretrained model!\")\n","\n","        # make summary\n","        tf.summary.scalar('MSE', self.mse)\n","        #tf.summary.scalar('SURE', self.sure)\n","        tf.summary.scalar('lr', self.lr)\n","        writer = tf.summary.FileWriter(self.log_dir+\"/\", self.sess.graph)\n","        merged = tf.summary.merge_all()\n","        summary_psnr = tf.summary.scalar('eva_psnr', self.eva_psnr)\n","        print(\"[*] Start training, with start epoch %d start iter %d : \" % (start_epoch, iter_num))\n","        start_time = time.time()\n","       # self.evaluate(eval_files, iter_num, summary_merged=summary_psnr, summary_writer=writer)\n","        tf.get_default_graph().finalize() # making sure that the graph is fixed at this point\n","        #training loop\n","        for epoch in range(start_epoch, epoch_):\n","            #print(\"Model: %s\" % (self.ckpt_dir))\n","            #print(\"Learning rate: {}\".format(lr[epoch]))\n","            \n","            rand_inds=np.random.choice(numData, numData,replace=False)\n","\n","            for batch_id in range(0, numBatch):\n","                # No RAM required\n","                batch_rand_inds = rand_inds[batch_id * batch_size:(batch_id + 1) * batch_size]\n","            #    batch_images            = np.array(gt_data[batch_rand_inds]).astype(np.float32) #/ 255.0     # clean gt\n","                batch_images_corrupt    = np.array(noisy[batch_rand_inds]).astype(np.float32) #/ 255.0       # noisy\n","                feed_dict = {#self.GT: batch_images,\n","                             self.Y: batch_images_corrupt,\n","                             self.lr: lr[epoch],\n","                             self.is_training: True}\n","                self.sess.run(self.train_op, feed_dict=feed_dict)\n","                \n","                if (iter_num)%100==0:\n","                    feed_dict2 = {#self.GT: batch_images,\n","                                 self.Y: batch_images_corrupt,\n","                                 self.lr: lr[epoch],\n","                                 self.is_training: False}\n","\n","                    mse,summary  = self.sess.run([self.mse,merged],feed_dict=feed_dict2)\n","                    print(\"Epoch: [%2d] [%4d/%4d] Time: %4.4f\"\n","                          % (epoch + 1, batch_id + 1, numBatch, time.time() - start_time))\n","                    writer.add_summary(summary, iter_num)\n","\n","                iter_num += 1\n","            if epoch==epoch_-1:\n","                self.save(iter_num, self.ckpt_dir)\n","            print('\\n')\n","        print(\"[*] Finish training.\")\n","\n","    def save(self, iter_num, ckpt_dir, model_name='CDnCNN-tensorflow'):\n","        checkpoint_dir = ckpt_dir\n","        if not os.path.exists(checkpoint_dir):\n","            os.makedirs(checkpoint_dir)\n","        print(\"[*] Saving model...\")\n","        self.saver.save(self.sess,\n","                   os.path.join(checkpoint_dir, model_name),\n","                   global_step=iter_num)\n","\n","    def load(self, checkpoint_dir):\n","        print(\"[*] Reading checkpoint...\")\n","        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n","        if ckpt and ckpt.model_checkpoint_path:\n","            full_path = tf.train.latest_checkpoint(checkpoint_dir)\n","            global_step = int(full_path.split('/')[-1].split('-')[-1])\n","            self.saver.restore(self.sess, full_path)\n","            return True, global_step\n","        else:\n","            return False, 0\n","          \n","    def test(self, save_dir):\n","        \"\"\"Test CDnCNN\"\"\"\n","        # init variables\n","        tf.initialize_all_variables().run()\n","\n","        if not os.path.exists(save_dir):\n","            os.makedirs(save_dir)\n","        \n","        load_model_status, global_step = self.load(self.ckpt_dir)\n","        assert load_model_status == True, '[!] Load weights FAILED...'\n","        print(\"[*] Load weights SUCCESS...\")\n","       \n","      #  clean = np.load(\"/content/drive/MyDrive/SURE_pstm/pstm_xiaobo.npy\")\n","      #  clean_image = clean[0:421,:,:,:]\n","        noisy = np.load(\"/content/drive/MyDrive/yyh/feature_select/test/kerry_440_400_50_1_W.npy\")\n","        print(noisy.shape)#(440, 400, 50, 1)\n","        \n","        #scio.savemat('noise.mat',{'ori_':noise})\n","        \n","        noise = np.reshape(noisy[0:440,0:400,0:50,0],[440,400,50])\n","        recover = np.zeros((440,400,50))\n","        j=0\n","        for i in range(0,440,440): #440\n","            noisy_image = noisy[i:i+440,:,:,:]\n","            output_clean_image = self.sess.run(self.Y_, feed_dict={self.Y: noisy_image, self.is_training: False})\n","            recover[j:j+440,:,:]= output_clean_image[:,:,:,0]\n","            j = j+440\n","        scio.savemat('/content/drive/MyDrive/yyh/feature_select/test/recover_440_400_50_W.mat',{'recover':recover})\n","        noisy = noisy[:,:,:,0]\n","        print(cal_psnr(recover, noisy))\n","        print(cal_psnr(noisy, recover))\n","\n","\n","\n","\n","#　Adjustment parameters\n","parser = argparse.ArgumentParser(description='') \n","parser.add_argument('--epoch', dest='epoch', type=int, default=15, help='# of epoch')\n","parser.add_argument('--batch_size', dest='batch_size', type=int, default=1, help='# images in batch')\n","parser.add_argument('--lr', dest='lr', type=float, default=0.0001, help='initial learning rate for sgd')#0.001\n","parser.add_argument('--sigma', dest='sigma', type=float, default=10.0, help='noise level (for evaluation and testing)')\n","\n","\n","parser.add_argument('--data', dest='data', default='/content/drive/MyDrive/SURE_htins/', help='training data path')\n","parser.add_argument('--checkpoint_dir', dest='ckpt_dir', default='/content/drive/MyDrive/SURE_htins/checkpoint', help='models are saved here')\n","parser.add_argument('--sample_dir', dest='sample_dir', default='./sample', help='sample are saved here')\n","parser.add_argument('--log_dir', dest='log_dir', default='./logs', help='tensorboard logs are saved here')\n","parser.add_argument('--test_dir', dest='test_dir', default='./test', help='test sample are saved here')\n","parser.add_argument('--eval_set', dest='eval_set', default='CBSD68', help='dataset for eval in training')\n","parser.add_argument('--test_set', dest='test_set', default='CBSD68', help='dataset for testing')\n","\n","parser.add_argument('--cost', dest='cost', default='SURE', help='cost to minimize: MSE, SURE, e-SURE')\n","parser.add_argument('--gpu', dest='gpu', default='0', help='which gpu to use')\n","parser.add_argument('--type', dest='type', default='blind_50_gt10', help='arg to give unique names to realizations')\n","parser.add_argument('--gt_type', dest='gt_type', default='5', help='arg to give unique names to realizations')\n","parser.add_argument('--phase', dest='phase', default='test', help='train or test')\n","parser.add_argument('--use_gpu', dest='use_gpu', type=int, default=1, help='gpu flag, 1 for GPU and 0 for CPU')\n","args = parser.parse_known_args()[0]\n","\n","\n","def denoiser_train(denoiser, lr):\n","    eval_files = glob('/content/drive/MyDrive/Extended_SURE/Dataset/testset/{}/*.png'.format(args.eval_set))\n","    denoiser.train(args.data, eval_files, batch_size=args.batch_size, epoch_=args.epoch, lr=lr, gt_type=args.gt_type)\n","\n","\n","def denoiser_test(denoiser, save_dir):\n","\n","    denoiser.test(save_dir)\n","\n","\n","def main(_):\n","    \n","    #the following string is attached to checkpoint, log and image folder names\n","    name = \"CDnCNN_\" + args.cost + '_' + str(args.type)\n","    \n","    ckpt_dir = args.ckpt_dir + \"/\" + name\n","    sample_dir = args.sample_dir + \"/\" + name\n","    test_dir = args.test_dir + \"/\" + name\n","    log_dir = args.log_dir + \"/\" + name\n","    print('CKPT path: ', ckpt_dir)\n","    \n","    if not os.path.exists(args.ckpt_dir):\n","        os.makedirs(args.ckpt_dir)\n","\n","    lr = args.lr * np.ones([args.epoch])\n","    lr[40:] = lr[0] / 10.0 #lr decay\n","\n","    \n","    if args.use_gpu:\n","        # added to control the gpu memory\n","        print(\"GPU\\n\")\n","        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n","        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n","            model = denoiser(sess, sigma=args.sigma, cost_str=args.cost, ckpt_dir=ckpt_dir, sample_dir=sample_dir, log_dir=log_dir)\n","            if args.phase == 'train':\n","                denoiser_train(model, lr=lr)\n","            elif args.phase == 'test':\n","                denoiser_test(model, test_dir)\n","            else:\n","                print('[!]Unknown phase')\n","                exit(0)\n","    else:\n","        print(\"CPU\\n\")\n","        with tf.Session() as sess:\n","            model = denoiser(sess, sigma=args.sigma, cost_str=args.cost, ckpt_dir=ckpt_dir, sample_dir=sample_dir, log_dir=log_dir)\n","            if args.phase == 'train':\n","                denoiser_train(model, lr=lr)\n","            elif args.phase == 'test':\n","                denoiser_test(model, test_dir)\n","            else:\n","                print('[!]Unknown phase')\n","                exit(0)\n","\n","\n","if __name__ == '__main__':\n","    os.environ['CUDA_VISIBLE_DEVICES']=str(args.gpu)\n","    tf.reset_default_graph()\n","    tf.app.run()\n","\n"],"metadata":{"id":"Ed5s8Tr48N2H","colab":{"base_uri":"https://localhost:8080/","height":360},"executionInfo":{"status":"error","timestamp":1666175823463,"user_tz":-480,"elapsed":6366,"user":{"displayName":"yan wang","userId":"02820627279848842686"}},"outputId":"d7e7e4a5-3a56-4bd3-dfc0-e4d87682c593"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["CKPT path:  /content/drive/MyDrive/SURE_htins/checkpoint/CDnCNN_SURE_blind_50_gt10\n","GPU\n","\n","[*] Initialize model successfully...\n"]},{"output_type":"stream","name":"stderr","text":["I1019 10:36:58.247167 139640794634112 saver.py:1412] Restoring parameters from /content/drive/MyDrive/SURE_htins/checkpoint/CDnCNN_SURE_blind_50_gt10/CDnCNN-tensorflow-6600\n"]},{"output_type":"stream","name":"stdout","text":["[*] Reading checkpoint...\n","[*] Load weights SUCCESS...\n","(440, 400, 50, 1)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]},{"output_type":"stream","name":"stdout","text":["21.219205123123107\n","21.06298211526681\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\n"]}]}]}